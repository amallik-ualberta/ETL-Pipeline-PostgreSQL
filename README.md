# ETL Pipeline using PostgreSQL

## Introduction
This repository contains code to build an ETL pipeline from raw JSON files using Python and PostgreSQL. A music-based startup called Sparkify collected data on songs and user activity on their music streaming app. To assist them make different analytical queries (e.g. Which is the most popular song in the past year?) on the data they have collected, I created a PostgreSQL database following the **Star Schema**. I implemented an ETL pipeline which transfers the data from JSON files into Postgres tables.

## How to Run

To create the Postgres tables and to load the data from the JSON files, simply run the bash file `run.sh`. Alternatively you could run the following 3 python files sequentially :

- sql_queries.py
- create_tables.py
- etl.py

## Dataset

The raw data for this project has been placed in the `data` folder, which has 2 subfolders `song_data` and `log_data`. 

The song dataset is a subset of real data from the [Million Songs Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. 

The second dataset consists of log files in JSON format generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## File Descriptions

- **sql_queries.py** : This file contains the necessary SQL commands to create tables, drop tables and insert data into the tables. Following the [Star Schema](https://en.wikipedia.org/wiki/Star_schema), I have created 4 dimension tables : **users, artists, songs, time** and 1 fact table **songplays**. The songplays table records log data associated with song plays, and contains **Foriegn Keys** to each of the dimension tables. The dimension tables present more detailed information about their respective entities.
- **create_tables.py** : This file creates the database, drops any pre-existing tables and creates new tables by running the commands provided in the `sql_queries.py` file
- **etl.py** : Iterates over each of the song and log files, extracts the required information and inserts them into the Postgres tables.
